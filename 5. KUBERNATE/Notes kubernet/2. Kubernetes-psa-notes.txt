(1) Basics :
------------

Q1. What is Kubernetes?
üëâ Kubernetes (K8s) is a free and open-source orchestration(üí° Orchestration = Managing multiple containers efficiently) tool developed by Google to manage containerized applications.


Q2. Why do we use Kubernetes?
üëâ Because it manages container lifecycles automatically ‚Äî handles :
                 - ‚úÖ Orchestration ‚Äì Efficiently manages multiple containers across a cluster of machines.
                 - ‚úÖ Self-Healing ‚Äì If a container crashes, Kubernetes automatically replaces it.
                 - ‚úÖ Load Balancing ‚Äì Distributes traffic across multiple containers to avoid overloading.
                 - ‚úÖ Auto Scaling ‚Äì Increases or decreases the number of running containers based on traffic load.
                 - ‚úÖ Automated Deployments ‚Äì Supports CI/CD for rolling updates and version control.

Q3. What problem does Kubernetes solve?
üëâ It eliminates manual container management ‚Äî such as restarting failed containers, distributing load, and coordinating deployments.

Q4. Explain Kubernetes Architecture?
üëâ Kubernetes follows a master‚Äìworker architecture: 
                              - Control Plane (Master):   API Server, etcd, Scheduler, Controller Manager. 
                              - Worker Node:    kubelet, kube-proxy, and container runtime (e.g., Docker or containerd).

Q5. What are the main components of the Control Plane(Master Node/Control Node)?
üëâ The control plane is responsible for managing the Kubernetes cluster. It includes the following components:
                       - API Server: Receives requests from kubectl and manages cluster operations.
                       - Scheduler: Identifies pending tasks in ETCD and assigns them to worker nodes.
                       - Controller Manager: Ensures the cluster‚Äôs desired state matches the actual state.
                       - ETCD: A distributed key-value store that acts as Kubernetes' internal database.


Q6. What are the main components of a Worker Nodes (Slave Nodes)?
üëâ Worker nodes run application workloads. They include the following components:
                       - Kubelet: A node agent that communicates with the control plane and manages containers.
                       - Kube Proxy: Manages networking and ensures communication within the cluster.
                       -  Docker Engine: Runs and manages containerized applications.
                       - Pod: The smallest deployable unit in Kubernetes, housing one or more containers.
                       - Container: Runs inside a Pod and contains the application code.

Q7. Explanation of Kubernetes working ?
      - Step 1: 
             - You interact with the Control Plane using kubectl (or other clients) to submit requests, like deploying an application.
      - Step 2: 
             - The API Server receives the request, validates it, and persists the desired state in etcd. The status may initially be ‚ÄúPending.‚Äù
      - Step 3: 
             - The Scheduler selects a suitable worker node (based on resource availability, taints, affinities, etc.) to run the workload.
      - Step 4: 
             - The Kubelet on the chosen worker node watches the API Server for assigned Pods and ensures containers are running as expected. 
      - Step 5: 
             - The Kube-Proxy manages networking rules on nodes to allow communication between Pods, Services, and external clients.
      - Step 6: 
             - The Controller Manager continuously monitors the cluster state and ensures the desired state is maintained (e.g., if a Pod fails, it will be                                                                                                                                                 rescheduled).

Q8. What is a Pod?
üëâ A Pod is the smallest unit you can run in Kubernetes. It is like a small box that can hold one or more containers that work closely together.
Note:- Pod, ReplicaSet, Deployment are separate concepts in Kubernetes. but heiraricaly these use  Deployment ‚Üí manages ReplicaSet ‚Üí manages Pods.


Q9. What is the difference between a Container and a Pod?
üëâ A container is like a single app running on its own, while a Pod is a small group that can hold one or more containers that share the same network and storage.

Q10. What is a ReplicaSet?
üëâ A ReplicaSet makes sure that a certain number of the same Pods are always running. If any Pod stops, it will create a new one to replace it.

Q11. What is a Deployment?
üëâ A Deployment manages ReplicaSets and provides declarative updates for pods (used for rolling updates and rollbacks).


Q12. What is a Service in Kubernetes?
üëâ A Service is a way to connect or communicate with Pods in Kubernetes. It gives a fixed name or address (IP/DNS) to reach the Pods, even if Pods are created or deleted. In simple words, it helps other apps or users find and talk to your Pods easily.

Q13. What are the different types of Kubernetes Services? 
       - ClusterIP (default): Internal-only communication. 
       - NodePort: Exposes service on each node‚Äôs IP. 
       - LoadBalancer: Uses cloud load balancer for external access. 
       - ExternalName: Maps service to external DNS name.


Q14. What is a Namespace?
üëâ A logical partition within a cluster to group resources (used for multi-environment or multi-team isolation).


Q15. What is a Label and Selector?
üëâ A Label is a small piece of information (key-value pair) you attach to an object, like a tag or name.
A Selector is used to find or group objects that have specific labels.


Q16. What is a ConfigMap? 
üëâ A ConfigMap stores non-sensitive configuration data (like environment variables or config files).


Q17. What is a Secret? 
üëâ A Secret stores sensitive data such as passwords or tokens (encoded in base64).

Q18. What is a Volume in Kubernetes? 
Volumes provide storage accessible to containers within a pod ‚Äî used for data persistence.

Q19. What is a Job and CronJob? 
    - Job: Runs a task to completion once. 
    - CronJob: Runs jobs on a schedule (like cron).

Q20. What is kubelet and kube-proxy?
    - kubelet: Manages pods on a node. 
    - kube-proxy: Manages networking rules for services and pods.

Q21. What is etcd?
üëâ etcd is a database that Kubernetes uses to store all its important information, like cluster settings and the current state of Pods, nodes, and other resources.

Q22. What is the difference between Kubernetes Master and Worker Node?
      - Master runs control-plane components; 
      - Worker runs application workloads (pods).

Q23. What is a YAML manifest file?
üëâ A configuration file that defines Kubernetes resources (pods, deployments, services, etc.)


Q24. What is kubectl? 
üëâ The command-line tool to interact with the Kubernetes cluster and manage resources.


Q25. What is the difference between kubectl apply, create, and replace? 
     - create: Creates a new resource. 
     - apply: Updates or creates resources declaratively. 
     - replace: Fully overwrites an existing resource.






(2) Installation & Setup :
-----------------------------

Q1. What are the prerequisites for installing Kubernetes?
        - A machine with Linux, Windows, or macOS.
        - Minimum 2 CPU cores and 2GB RAM for single-node clusters.
        - Docker or any container runtime installed.
        - kubectl command-line tool installed.


Q2. How to install Minikube?
     - Minikube is a tool to run Kubernetes locally.
     - Installation (Linux/macOS/Windows) via package manager or binary. 
Example for Linux:        curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
                          sudo install minikube-linux-amd64 /usr/local/bin/minikube



Q3.What is kubectl and how to configure it?
üëâ kubectl is a command-line tool used to communicate with and manage a Kubernetes cluster. It helps you deploy, view, and control Kubernetes resources like Pods, Deployments, and Services.
      Why we update or configure kubectl?
                       - kubectl uses a kubeconfig file to know which cluster to talk to and how to authenticate. This file has:
                       - Cluster address (where the Kubernetes API server is)
                       - User credentials (how kubectl can access the cluster)
                       - Context (which cluster and user kubectl should use)
                       - You usually don‚Äôt need to touch this file if your cluster setup doesn‚Äôt change.
         But you need to update or reconfigure kubectl when:
                       - You create a new cluster ‚Üí kubectl needs to know the new cluster details.
                       - Cluster configuration changes ‚Üí for example, cluster IP, certificate, or user credentials change.
                       - You want to switch between clusters ‚Üí kubectl needs the correct context to point to the cluster you want to work with.
STEP 1- Install kubectl
            For Linux: 
                 curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-                                        release/release/stable.txt)/bin/linux/amd64/kubectl"
       Note:- Download the latest version of the kubectl binary for Linux 64-bit and save it in the current directory with the same filename (kubectl).
               chmod +x kubectl
Note:- it is used to change file permissions like, here used for kubectl file permission to execute.
              sudo mv kubectl /usr/local/bin/
Note:- This one‚Äôs about moving the kubectl file into a system-wide executable location so you can run it from anywhere.
 STEP 2- Get and Set Up the Cluster Configuration (kubeconfig file) :
         - The kubeconfig file contains cluster connection details like the cluster address, credentials, and context.
         - You usually get this file from your cluster admin or cloud provider.
                                   example : 
                                        - For Minikube: minikube update-context
                                        - For AWS EKS: aws eks update-kubeconfig --region <region-name> --name <cluster-name>
                        Then, place the kubeconfig file in the default location:
                                                          - Linux/Mac: ~/.kube/config
                                                          - Windows: C:\Users\<your-username>\.kube\config
STEP 3- Verify kubectl setup : kubectl version -o yaml
STEP4- Then test the connection with the cluster: kubectl get nodes


Q4. How to start and stop Minikube?
minikube start   # Starts the cluster
minikube stop    # Stops the cluster

Q5. How to verify the cluster setup?
kubectl get nodes      # Lists nodes
kubectl cluster-info   # Shows cluster info
kubectl get pods --all-namespaces


Q6. How to deploy your first app in Minikube?
kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.10
kubectl expose deployment hello-node --type=NodePort --port=8080
minikube service hello-node





(3) Pods & Deployments (Core Operations) :
----------------------------------------------





                                       




üëâ 
Kubernetes Advantages (Detailed Explanation)
1) Orchestration ‚Äì Managing Containers
Kubernetes helps manage multiple Docker containers across different machines (nodes) efficiently.

üîπ Instead of running docker run manually for each container, Kubernetes automates deployment.
üîπ It ensures that all containers are running smoothly and adjusts their status as needed.

Note: Kubernetes ensures all these containers are running, healthy, and communicating with each other properly.

2) Self-Healing ‚Äì Automatic Recovery
If a container crashes due to an error or system failure, Kubernetes automatically restarts a new instance.

üìå Example:

A web server container (Apache, Nginx) stops unexpectedly.

Kubernetes detects the failure and starts a new container to replace it.

Users never notice downtime.

3) Load Balancing ‚Äì Distributes Traffic Efficiently
Kubernetes distributes incoming user requests across multiple containers to avoid overloading any single instance.

üìå Example:

A shopping website experiences high traffic during a sale.

Kubernetes ensures that requests are evenly distributed across available backend servers.

Prevents server crashes and ensures smooth performance.

4) Auto Scaling ‚Äì Adjusting Resources Dynamically
Kubernetes can increase or decrease the number of containers automatically based on traffic load.

üìå Example:

If website traffic increases, Kubernetes adds more containers to handle the load.

If traffic reduces, Kubernetes removes extra containers to save resources.

Works similarly to cloud-based Auto Scaling Groups (ASG).

########################################################
(3) Conclusion (difference between docker & kubernets)
########################################################
üöÄ Docker simplifies packaging applications into portable containers.
üöÄ Kubernetes ensures these containers are orchestrated, scalable, and reliable.

Together, Docker & Kubernetes enable modern cloud-native application deployment‚Äîmaking applications highly available, efficient, and automated.










###################################
(6) Kubernetes (K8s) Cluster Setup
####################################

A Kubernetes Cluster is a group of servers working together to run containerized applications. It can be set up in two main ways:

A Kubernetes Cluster = Control Plane + Worker Nodes + Pods + Resources + Networking + Storage

1) Self-Managed Cluster 
In this setup, we manually install and manage Kubernetes on our own infrastructure.

a) MiniKube (Single Node)
-> Runs a single-node cluster on a local machine.
-> Best for learning and practicing Kubernetes concepts.
-> Not suitable for production as it lacks high availability and scalability.

b) Kubeadm (Multi-Node)
-> A tool for setting up a multi-node cluster manually.
-> Requires configuring the control plane, worker nodes, and networking.
-> Gives full control over the cluster but requires deep Kubernetes expertise.
-> Used for on-premise or customized Kubernetes deployments.

2) Cloud Provider-Managed Cluster (Pre-configured, ready-to-use)
Cloud providers offer fully managed Kubernetes services, where they handle cluster maintenance, updates, and availability.

a) AWS EKS (Elastic Kubernetes Service)
-> A managed Kubernetes service on Amazon Web Services.

b) Azure AKS (Azure Kubernetes Service)
-> Microsoft Azure‚Äôs managed Kubernetes offering.


c) GCP GKE (Google Kubernetes Engine)
-> Google Cloud‚Äôs fully managed Kubernetes solution.

#########################
(7) MiniKube Setup
#######################

Step-1 : Setup Linux VM

Login into AWS Cloud account
Create Linux VM with Ubuntu AMI - t2.medium
Select Storage as 50 GB or more with 2 vCPU required minimum(Default is 8 GB only for Linux)
Create Linux VM and connect to it using SSH Client


Step-2 : Install Docker In Ubuntu VM

sudo apt update
curl -fsSL get.docker.com | /bin/bash
sudo usermod -aG docker ubuntu 
exit

Step-3 : Updating system packages before installing Minikube dependencies

sudo apt update
sudo apt install -y curl wget apt-transport-https

Step-4 : Installing Minikube

curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
minikube version

Step-5 : Install Kubectl (Kubernetes Client)

curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
kubectl version -o yaml

Step-6 : Start MiniKube Server

minikube start ‚Äî driver=docker

Step-7 : Check MiniKube status

minikube status

Step-8 : Access K8S Cluster

kubectl cluster-info

Step-9 : Access K8S Nodes

kubectl get nodes


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ Setup Completed $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#####################################
üöÄ (8) What is a POD in Kubernetes?
#####################################

‚úÖ Key Concepts Explained:
-----------------------------
1. If you deploy an app, it will ultimately run inside one or more Pods. It is a building block to run app that we deploy in K8S

2. "Applications will be deployed as PODS in k8s."
Your app (e.g., a Spring Boot API) will be containerized using Docker. This container will then be wrapped inside a Pod and deployed on the cluster.

3. "To create PODS we will use Docker images."
A Pod runs one or more containers (usually one), and each container uses a Docker image. You can build a Docker image of your app and then deploy it inside a Pod.

4. "To create PODS we will use Manifest YML file."
A YAML manifest file defines the configuration for the Pod (or other resources like Deployments).

It includes:

a. The name of the Pod

b. The image to use

c. Ports to expose

c. Environment variables, etc.


5. "Create multiple PODS."
The same image (e.g., myapp:latest) can be used to create many Pods. This is how you scale your application‚Äîrunning multiple copies (Pods) to handle more traffic.

6. "If we run application with multiple pods then Load Balancing can be performed resulting in 99.9% uptime of the application."
High Availability: If one Pod crashes, others are still running, so your app stays available.Load Balancing: Kubernetes distributes traffic across Pods using a Service (like a load balancer).

7. "PODS count will be increased and decreased based on the demand (scalability)."
Kubernetes supports auto-scaling. You can scale Pods manually or automatically using a Horizontal Pod Autoscaler (HPA). Based on CPU/memory usage or custom metrics, Kubernetes will increase/decrease the number of Pods.


###############################
üöÄ (9) Kubernetes Services
###############################

-> A Kubernetes Service is used to expose a group of Pods so that they can be accessed reliably. Since Pods can be created and destroyed at any time (with changing IPs), a Service gives them a stable network identity.

---------------------------
üß≠ Why Do We Use Services?
---------------------------

-> Pods are short-lived and can crash or restart.

-> Each time a Pod is created, it gets a new IP address.

-> Directly accessing Pods via IP is not reliable.

-> A Service gives a static IP to a group of Pods.

--------------------------------------
üåê (10) Types of Kubernetes Services
--------------------------------------

Kubernetes offers different types of services depending on how and where you want to expose your Pods:

üîπ 1. ClusterIP (Default)
üî∏ 2. NodePort
üîπ 3. LoadBalancer

-------------------------------------------------------
üîê (11) ClusterIP Service (Internal Access Only)
-------------------------------------------------------

üìå Key Points:
-> Pods are short-lived objects; if one crashes, Kubernetes replaces it with a new Pod.
-> Every new Pod gets a different IP address.

Note: üõë Never rely on Pod IPs to access an application.

-> A ClusterIP Service groups multiple Pods  and assigns them a single static IP.

-> This static IP allows other components inside the cluster to access the group of Pods reliably, even when individual Pods change.

---------------------
üö´ Access Scope:
---------------------

-> Only accessible within the Kubernetes cluster.

-> Not reachable from the outside world (internet or external clients).

----------------
üí° Use Case:
----------------
-> Internal services such as databases, backend APIs, authentication services, etc.

Example: You don‚Äôt want to expose a database Pod to the internet, so you use a ClusterIP service to allow access only from other internal Pods.

----------------------------------------------------
üåê (12) What is a NodePort Service in Kubernetes?
----------------------------------------------------
-> A NodePort service is a type of Kubernetes Service that exposes your Pods outside the cluster using a port on each worker node (called a "NodePort").

üß≠ Why Use NodePort?
------------------------------------
By default, Pods and ClusterIP services are only accessible within the cluster.

NodePort makes them accessible externally by opening a static port (from 30000 to 32767) on each worker node.

It allows you to access your application using:

http://<NodeIP>:<NodePort>


Note: Here all traffic is routed to one worker node. Means load balancing cannot happen here.

-----------------------------------------------------
üåê (13) What is a LoadBalancer Service in Kubernetes?
-----------------------------------------------------

-> It not only provides external access to your app but also handles automatic traffic distribution across the backend Pods running on different worker nodes.


##############################################
üìÑ (14) What is a Kubernetes Manifest YAML?
##############################################

-> Think of it as an instruction manual for Kubernetes to create and manage resources.

* EXTRA : for validiting yml indentation use this url : https://www.yamllint.com/

üß± Main Sections of a Manifest YAML
-----------------------------------
Here are the 4 main parts:

apiVersion: <version-number>   # API version to use
kind: <resource-type>          # Type of resource (Pod, Service, Deployment, etc.) 
metadata:                      # Metadata like name, labels
spec:                          # Specification of what the resource should do

-------------------------------
üß™ Example: Pod Manifest YAML
Let‚Äôs look at a simple Pod definition:
-------------------------------

---
apiVersion: v1
kind: Pod
metadata:
  name: testpod
  labels:
    app: dempapp
spec:
  containers:
    - name: test
      image: psait/pankajsiracademy:latest
      ports:
        - containerPort: 9090
...

-------------------
Explanation
-------------------

apiVersion: v1
Tells Kubernetes to use version v1 of the API.

Since you are creating a Pod, this is the correct API version.

kind: Pod
Defines the type of resource you want to create.

In this case, it‚Äôs a Pod, which is the smallest and simplest unit in Kubernetes.

metadata:
Metadata gives Kubernetes basic info about your Pod.

name: testpod
This is the name of your Pod.

You‚Äôll use this name to check logs or status (e.g., kubectl get pod testpod).

labels:
Labels are key-value pairs to categorize and group Kubernetes objects.

app: dempapp is a label to help identify this Pod as part of the "dempapp" application.

spec:
This section defines what‚Äôs inside the Pod.

containers:
A Pod can contain one or more containers. You're defining one container here.

- name: test
This is the name of the container inside the Pod (not the Pod itself).

image: psait/pankajsiracademy:latest
This is the Docker image used to create the container.

It will pull the latest version of psait/pankajsiracademy from Docker Hub or another registry.

‚ö†Ô∏è Make sure the image exists and is accessible (public or with correct credentials).

ports:
This tells Kubernetes the container is listening on port 8080 inside the Pod.

containerPort: 8080
This is the internal port your application is running on.

Kubernetes can use this for things like service routing, health checks, etc.

#############
(15) Commands:
######################

Note: Save above content in .yml file

# execute manifest yml
kubectl apply -f <manifest-yml-file>

# check pods
kubectl get pods

# check pod logs
kubectl logs <pod-name>

# Describe pods
kubectl describe pod <pod-name>


---------------------------------------------
K8s Service Manifest YAML (for your Pod)
------------------------------------------

---
apiVersion: v1
kind: Service
metadata:
  name: testpod-service
spec:
  type: NodePort
  selector:
    app: dempapp           # This must match the Pod's label
  ports:
    - port: 80             # Exposed port for external access
      targetPort: 9090     # Port on which the app is running inside the container
      nodePort: 30080      # External port exposed on each node


üí° Explanation:
----------------------
name: testpod-service ‚Äì The name of the service.

type: NodePort ‚Äì Exposes the Pod outside the cluster.

selector.app: dempapp ‚Äì This matches the label of your Pod, so the service knows which Pod(s) to route to.

port: 80 ‚Äì The port used when calling the service.Port 80 is the default port for HTTP traffic

You're using a web server like Nginx, Apache, or similar.

targetPort: 8080 ‚Äì The port your container app actually listens on.

nodePort: 30080 ‚Äì External port accessible via http://<NodeIP>:30080

Commands
___________


# üì¶ Create the service using the YAML
kubectl apply -f testpod-service.yml

# üîÅ Verify that the service is created /  Check existing services
kubectl get svc


# üö™ Open service in browser (Minikube only)
minikube service testpod-service


# for Test
#Get address

minikube ip


Test this in same local network

curl http://<mini-kube-ip>:3080/
curl http://192.168.49.2:30080/



Part	Meaning
curl	A tool to make HTTP requests from the command line. It's often used to test whether a URL is reachable and what it returns.
http://192.168.49.2	This is the IP address of the Minikube VM. It's the entry point into your Kubernetes cluster from your host machine. You found this IP using minikube ip.
:30080	This is the NodePort exposed by your Kubernetes service (testpod-service). It forwards external requests to the internal Pod‚Äôs port (8080 in your case).
/	This is the path of the URL. Since it's just a /, it hits the root endpoint of your Spring Boot app.


#################################################
note: How to delete pod and services

kubectl delete pod testpod
kubectl delete svc testpod-service

kubectl apply -f pod-01.yml
kubectl apply -f service-01.yml
############################################

#############
Stop complete minikube
#################

-> minikube stop
-> minikube delete
-> minikube status

#####################
To see all resources running
####################

-> kubectl get all

########################
To delete all resources use
########################
-> kubectl delete all --all


#######################################
(16) What are name spaces in k8s?
#######################################

-> They help logically group and isolate resources. Just like how we create folders to isolate our work in computers.

------------------
Example:
-----------------

database-ns = all database-related stuff

backend-ns = for backend applications

Note: If we donot specifiy name space they k8S will automatically provide default name - space

#####################
Commands
###################

list all name spaces
----------------------
-> kubectl get ns

list all pods in given name space
-----------------------------------
-> kubectl get pods -n <name-space>

#############################################
(17) How to create name space in k8s
#############################################

1. Using kubectl command-
kubectl create namespace backend-ns

2.using manifest yml file 


---
apiVersion: v1
kind: Namespace
metadata:
 name: backend-ns
... 

# execute manifest yml
kubectl apply -f <yml-file-name>

# get all resources belongs to backend-ns namespace
kubectl get all -n backend-ns


#get all worker nodes
kubectl get nodes

#delete name space - All resource related to that will be deleted
kubectl delete ns backend-ns

#Open tunnel
minikube service <service-name>



############################
Namespace with POD with Service creation yml file
##############################
---
apiVersion: v1
kind: Namespace
metadata:
 name: backend-ns
---
apiVersion: v1
kind: Pod
metadata:
  name: testpod
  namespace: backend-ns
  labels:
    app: dempapp
spec:
  containers:
    - name: test
      image: psait/pankajsiracademy:latest
      ports:
        - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
  name: testpod-service
  namespace: backend-ns
spec:
  type: NodePort
  selector:
    app: dempapp           # This must match the Pod's label
  ports:
    - port: 80             # Exposed port for external access
      targetPort: 9090     # Port on which the app is running inside the container
      nodePort: 30080      # External port exposed on each node
...


#########################################
(18) k8S Resources
########################################

-> When you create a Pod directly using kind: Pod, Kubernetes does not manage its lifecycle ‚Äî if it crashes or is deleted, it's gone forever unless recreated manually.

-> K8S resources manages POD lifecycle

-> To let Kubernetes manage, restart, and scale Pods, we use higher-level controllers like the ones you listed.

üîÅ 1) ReplicationController (RC)
üîÅ 2) ReplicaSet (RS)
üöÄ 3) Deployment
üõ∞ 4) DaemonSet
üíæ 5) StatefulSet

#####################################
üì¶ (19) What is ReplicationController (RC)?
A Kubernetes resource used to manage the lifecycle of Pods.

Ensures a specified number of Pods are always running.

Provides self-healing ‚Äî if a Pod crashes or is deleted, RC will recreate it.

manifest yml file

---
apiVersion: v1
kind: ReplicationController
metadata:
 name: webapp
spec:
 replicas: 3
 selector:
  app: dempapp
 template:
  metadata:
   name: testpod
   labels: 
    app: dempapp
  spec:
   containers:
    - name: webappcontainer
      image: psait/pankajsiracademy:latest
      ports:
      - containerPort: 9090
...

kubectl apply -f rc.yml

########################################################

kubectl get all

kubectl get pods

kubectl delete pod <pod-name>

kubectl get pods

kubectl scale rc demoapp --replicas=5

kubectl scale rc demoapp --replicas=1  Explain in short

##############################################

‚úÖ (20)ReplicaSet in Kubernetes
===================
üîπ What is a ReplicaSet?
A ReplicaSet (RS) is a Kubernetes resource that ensures a specified number of identical Pods are running at any given time.

üí° Key Features:
Self-healing: If a Pod crashes or is manually deleted, the RS will automatically create a new Pod to maintain the desired number.

Scaling: You can increase or decrease the number of replicas (Pods) easily.

Selector-based matching: RS manages only those Pods that match its label selector.

Example: 

# -------- Manually created Pod with label app: myapp --------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: myappcontainer
      image: nginx
      ports:
        - containerPort: 80

---
# -------- Manually created Pod with label app: dempapp --------
apiVersion: v1
kind: Pod
metadata:
  name: dempapp-pod
  labels:
    app: dempapp
spec:
  containers:
    - name: dempappcontainer
      image: nginx
      ports:
        - containerPort: 80

---
# -------- ReplicaSet that manages both app: dempapp and app: myapp --------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-webapp
spec:
  replicas: 2
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - dempapp
          - myapp
  template:
    metadata:
      labels:
        app: dempapp   # Pods created by RS will have this label
    spec:
      containers:
        - name: webappcontainer
          image: nginx
          ports:
            - containerPort: 80
-----------------------------------------------------------------------------------------------------


replica-set.yml for practicals
------------------

---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demoapp         # Must match pod template labels
  template:
    metadata:
      labels:
        app: demoapp
    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090

---
apiVersion: v1
kind: Service
metadata:
  name: webappservice
spec:
  type: NodePort
  selector:
    app: demoapp           # Must match pod labels
  ports:
    - port: 80
      targetPort: 9090
      nodePort: 30095
######################################
‚úÖ (21) Deployment in Kubernetes
######################################

(22) difference bt ReplicaSet & Deployement
--------------------------------------------
Feature	               ReplicaSet	                  Deployment
Manages Pods	            ‚úÖ Yes	                    ‚úÖ Yes (via ReplicaSet)
Rolling Updates	            ‚ùå No	                    ‚úÖ Yes
Rollbacks	            ‚ùå No	                    ‚úÖ Yes
YAML Kind	            ReplicaSet	                    Deployment
Use in Real World	    Rare	                    Very Common


(21 start again from here..)

A Deployment in Kubernetes is one of the most recommended and used resources for managing Pod lifecycles. It ensures reliable application deployment with features like zero downtime, auto-scaling, rolling updates, and rollback capabilities.

üéØ Key Advantages of Using Deployments
Zero Downtime: Deployments ensure high availability by using strategies like rolling updates. Even when pods are being updated, the service remains available to users.

Auto Scaling: With Kubernetes Horizontal Pod Autoscaler, you can automatically scale your Pods based on CPU or memory usage (or other custom metrics).

Rolling Update & Rollback: Kubernetes allows rolling updates, which means it will gradually update Pods one by one, ensuring the application remains available throughout the process. If something goes wrong during the update, you can rollback to a previous stable version.

üë®‚Äçüíª When to Choose Which Strategy?
Use RollingUpdate:

For most production workloads where high availability and zero downtime are required.

When you are gradually releasing new versions of your application and want to avoid service disruption.

Use Recreate:

For non-critical applications or during maintenance windows where it's okay to have a temporary outage.

When you need to clear everything and redeploy fresh Pods (e.g., clearing persistent state or major upgrades).

############################
deployment-service.yml
#########################
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dempapp         # Must match pod template labels
  template:
    metadata:
      labels:
        app: dempapp
    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
  name: webappservice
spec:
  type: NodePort
  selector:
    app: dempapp           # Must match pod labels
  ports:
    - port: 80
      targetPort: 9090
      nodePort: 30095

#################################################################################################################

üîß 1. Deployment

apiVersion: apps/v1
Uses the apps/v1 API to define a Deployment.


kind: Deployment
Declares that this is a Deployment (used to manage pods).


metadata:
  name: webapp
Assigns the Deployment a name: webapp.


spec:
  replicas: 3
Tells Kubernetes to run 3 replicas (3 pods) of your app.

  selector:
    matchLabels:
      app: dempapp
The deployment will manage pods that have the label app: dempapp.


  template:
    metadata:
      labels:
        app: dempapp
This is the pod template. Every pod created will be labeled app: dempapp.

This must match the selector above.

    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090

This is the container spec inside the pod:

name: webappcontainer

image: Docker image hosted at Docker Hub (psait/pankajsiracademy:latest)

containerPort: The app runs on port 9090 inside the container.

üåê 2. Service

apiVersion: v1
Uses the core v1 API to define a Service.


kind: Service
Declares a Service (used to expose pods).


metadata:
  name: webappservice
The service name is webappservice.


spec:
  type: NodePort
NodePort type: Exposes your app externally by opening a port on every node in the cluster.

Clients can access it via:
http://<NodeIP>:30095


  selector:
    app: dempapp
The service targets pods with label app: dempapp ‚Äî which matches the Deployment.


  ports:
    - port: 80
      targetPort: 9090
      nodePort: 30095
port: 80: The port the service receives traffic on (internally).

targetPort: 9090: Forwards traffic to container's port 9090.

nodePort: 30095: Opens this port on the Kubernetes node ‚Äî used to access the service from outside the cluster.


################################
Deployment with load balancer
###############################

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dempapp         # Must match pod template labels
  template:
    metadata:
      labels:
        app: dempapp
    spec:
      containers:
        - name: webappcontainer
          image: psait/pankajsiracademy:latest
          ports:
            - containerPort: 9090

---
apiVersion: v1
kind: Service
metadata:
  name: webappservice
spec:
  type: LoadBalancer     # ‚úÖ Changed from NodePort to LoadBalancer
  selector:
    app: dempapp
  ports:
    - port: 80
      targetPort: 9090


Note: 
                                                                       -----------------STARTING EKS SERVICE FROM HERE CREATING QUEZ FROM HERE -----------------------------------------
üß† First: Is Each Worker Node a Separate VM?
‚úÖ Yes, each worker node in EKS is an individual EC2 instance (VM).

So, if you have 4 worker nodes, EKS will launch 4 separate EC2 instances in your account (under the hood).

üöÄ How to Create More Worker Nodes in EKS?
There are two ways to create more worker nodes:

‚úÖ Option 1: CLI Method (Add --nodes flag)
You can specify the number of nodes when creating the cluster using the --nodes flag:

eksctl create cluster \
  --name psait-cluster4 \
  --region ap-south-1 \
  --node-type t2.medium \
  --zones ap-south-1a,ap-south-1b \
  --nodes 4 \
  --nodes-min 2 \
  --nodes-max 6
üîç What This Means:
--nodes 4 ‚Üí Start with 4 worker nodes

--nodes-min 2 ‚Üí Minimum nodes for autoscaling

--nodes-max 6 ‚Üí Maximum nodes for autoscaling

Nodes will be spread across the AZs you mention (load balanced)

‚úÖ Option 2: YAML Config File (More Flexible)
Create a cluster.yaml like this:

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: psait-cluster4
  region: ap-south-1

availabilityZones: ["ap-south-1a", "ap-south-1b"]

nodeGroups:
  - name: ng-1
    instanceType: t2.medium
    desiredCapacity: 4
    minSize: 2
    maxSize: 6
    volumeSize: 20

Note: We can use terrform for the same to create EKS setup


#####################################
        HOW TO SETUP EKS ?
#####################################

# Step - 1 : Create EKS Management Host in AWS #

1) Launch new Ubuntu VM using AWS Ec2 ( t2.micro )
	  
2) Connect to machine and install kubectl using below commands  

curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin
kubectl version --short --client


3) Install AWS CLI latest version using below commands 

sudo apt install unzip
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version



4) Install eksctl using below commands

curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
eksctl version





# Step - 2 : Create IAM role & attach to EKS Management Host #


1) Create New Role using IAM service ( Select Usecase - ec2 ) 
	
2) Add below permissions for the role 
	- Administrator - acces 
		
3) Enter Role Name (eksrole) 

4) Attach created role to EKS Management Host (Select EC2 => Click on Security => Modify IAM Role => attach IAM role we have created) 






# Step - 3 : Create EKS Cluster using eksctl # 
**Syntax:** 

eksctl create cluster --name cluster-name  \
--region region-name \
--node-type instance-type \
--nodes-min 2 \
--nodes-max 2 \ 
--zones <AZ-1>,<AZ-2>


EXAMPLE OF ABOVE SYNTAX:

## EX-1: N. Virgina: 
eksctl create cluster --name psait-cluster4 --region us-east-1 --node-type t2.medium  --zones us-east-1a,us-east-1b
	
## EX-2:  Mumbai: 
eksctl create cluster --name psait-cluster4 --region ap-south-1 --node-type t2.medium  --zones ap-south-1a,ap-south-1b


## After cluster created we can check nodes using below command.
 kubectl get nodes 
also navigate to AWS console to see cluster like, search EKS -> cluster and see also varify Running instance will find 2 more ec2 instance.

Note: We should be able to see EKS cluster nodes here.And We are done with our Setup #





	
# Step - 4 : After your practise, delete Cluster and other resources we have used in AWS Cloud to avoid billing #
eksctl delete cluster --name psait-cluster4 --region ap-south-1


Note: just after above setup use bellow file by creating named, vi deployment.yml

yml file to deploy application in eks with loadbalancer
#############################################################
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: webapp
spec:
 replicas: 2
 strategy: 
  type: RollingUpdate
 selector:
  matchLabels:
   app: javawebapp
 template:
  metadata:
   name: javawebpod
   labels:
    app: javawebapp
  spec:
   containers:
   - name: webappcontainer
     image: psait/pankajsiracademy:latest
     ports:
     - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
 name: websvc
spec:
 type: LoadBalancer
 selector:
  app: javawebapp
 ports:
  - port: 80
    targetPort: 9090
...

Now Run the commands:
-> kubectl apply -f deployment.yml , now go to ec2 dashboard and reload find loadbalancer 1, click on it find DNS url +/message
-> also unable 80 


#########################################################################
                        EKS WITH JENKINS PIPE LINE
########################################################################


# Creating Ci/CD pipeline using following tools
1) Maven
2) Git Hub
3) Jenkins
4) Docker
5) Kubernetes





# Step - 1 : Create EKS Management Host in AWS #

1) Launch new Ubuntu VM using AWS Ec2 ( t2.micro )	  
2) Connect to machine and install kubectl using below commands :  
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin
kubectl version --short --client

3) Install AWS CLI latest version using below commands :
sudo apt install unzip 
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version

4) Install eksctl using below commands :
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
eksctl version





# Step - 2 : Create IAM role & attach to EKS Management Host & Jenkins Server #

1) Create New Role using IAM service ( Select Usecase - ec2 ) 	
2) Add below permissions for the role
	- IAM - fullaccess
	- VPC - fullaccess
	- EC2 - fullaccess
	- CloudFomration - fullaccess
	- Administrator - access
		
3) Enter Role Name (eksrole) 
4) Attach created role to EKS Management Host (Select EC2 => Click on Security => Modify IAM Role => attach IAM role we have created) 
5) Attach created role to Jenkins Machine (Select EC2 => Click on Security => Modify IAM Role => attach IAM role we have created) : THIS AFTER JENKINS SERVER CREATED.
 
 




# Step - 3 : Create EKS Cluster using eksctl # 
**Syntax:** 

eksctl create cluster --name cluster-name  \
--region region-name \
--node-type instance-type \
--nodes-min 2 \
--nodes-max 2 \ 
--zones <AZ-1>,<AZ-2>

EXAPLE OF ABOVE SYSNTAX: 
eksctl create cluster --name psait-cluster --region ap-south-1 --node-type t2.medium  --zones ap-south-1a,ap-south-1b
USED THIS COMMAND TO VARIFY :
kubectl get nodes  





# Step-4 : Jenkins Server Setup in Linux VM #

1) Create Ubuntu VM using AWS EC2 (t2.medium)
2) Enable 8080 Port Number in Security Group Inbound Rules
3) Connect to VM using MobaXterm
4) Install Java :
sudo apt update
sudo apt install fontconfig openjdk-17-jre
java -version


5) Install Jenkins :

sudo wget -O /usr/share/keyrings/jenkins-keyring.asc \
  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key
echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null
sudo apt-get update
sudo apt-get install jenkins


6) Start Jenkins

sudo systemctl enable jenkins
sudo systemctl start jenkins


7) Verify Jenkins

sudo systemctl status jenkins


	
8) Open jenkins server in browser using VM public ip

http://public-ip:8080/


9) Copy jenkins admin pwd

sudo cat /var/lib/jenkins/secrets/initialAdminPassword


	   
10) Create Admin Account & Install Required Plugins in Jenkins





## Step-5 : Configure Maven as Global Tool in Jenkins ##

1) Manage Jenkins -> Tools -> Maven Installation -> Add maven






## Step-6 : Setup Docker in Jenkins ##

curl -fsSL get.docker.com | /bin/bash
sudo usermod -aG docker jenkins
sudo systemctl restart jenkins
sudo docker version






# Step - 8 : Install AWS CLI in JENKINS Server #

URL : https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html (NOT NOW NEED ) 

**Execute below commands to install AWS CLI**
sudo apt install unzip 
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version

 



# Step - 9 : Install Kubectl in JENKINS Server #

**Execute below commands in Jenkins server to install kubectl**
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin
kubectl version --short --client


NOTE: From bellow steps are dependent on each other server for communication because host server going to setup kubernets cluster,after getting all kubernetes 
details here like kubernetes configuration file, take this file and give the jenkins server then jenkins server should have the configuration details of kubernetes
the jenkins server manages your kubernetes ? 




# Step - 10 : Update EKS Cluster Config File in Jenkins Server #
	
1) Execute below command in Eks Management host & copy kube config file data
	$ cat .kube/config 

2) Execute below commands in Jenkins Server and paste kube config file
	$ cd /var/lib/jenkins
	$ sudo mkdir .kube
	$ sudo vi .kube/config

3) Execute below commands in Jenkins Server and paste kube config file for ubuntu user to check EKS Cluster info
	$ cd ~
	$ ls -la
	$ sudo vi .kube/config

4) check eks nodes :
	$ kubectl get nodes 

**Note: We should be able to see EKS cluster nodes here.**








# Step - 11 : Create Jenkins CI CD Job #

Note: before creating pipeline in your project add .yml in root path having content bellow:

---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: webapp
spec:
 replicas: 2
 strategy: 
  type: RollingUpdate
 selector:
  matchLabels:
   app: javawebapp
 template:
  metadata:
   name: javawebpod
   labels:
    app: javawebapp
  spec:
   containers:
   - name: webappcontainer
     image: psait/pankajsiracademy:latest
     ports:
     - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata:
 name: websvc
spec:
 type: LoadBalancer
 selector:
  app: javawebapp
 ports:
  - port: 80
    targetPort: 9090
...



Now go to Jenkins and create pipe line using bellow steps:

- **Stage-1 : Clone Git Repo** 
- **Stage-2 : Maven Build** 
- **Stage-3 : Create Docker Image** 
- **Stage-4 : Push Docker Image to Registry** 
- **Stage-5 : Deploy app in k8s eks cluster** 

EXAMPLE FOR PIPE LINE:

pipeline {
    agent any
    
    tools{
        maven "Maven-3.9.9"
    }

    stages {
        stage('Clone Repo') {
            steps {
                git 'https://github.com/pankajmutha14/docker-test.git'
            }
        }
        stage('Maven Build') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Docker Image') {
            steps {
                sh 'docker build -t psait/pankajsiracademy:latest .'
            }
        }
        stage('k8s deployment') {
            steps {
                sh 'kubectl apply -f k8s-deploy.yml'
            }
        }
    }
}

	
# Step - 12 : Access Application in Browser #
- **We should be able to access our application, go to loadbalancers and take DNS url** <br/>
URL : http://LBR/context-path/
	







##############################
Scaling in Kubernetes
#############################

1. . HPA - Horizontal Pod Autoscaler
###########################################
What it does: Adds or removes pods based on CPU/memory usage, custom metrics, or external metrics.

Use case: When traffic increases, Kubernetes spins up more pods to handle the load.

Controlled by: The HorizontalPodAutoscaler object.

Example: If CPU usage goes above 80%, HPA can increase the pod count from 2 to 5 automatically

 2. VPA - Vertical Pod Autoscaler
What it does: Adjusts CPU and memory requests/limits of a pod automatically.

Apply Load in HPA
--------------
kubectl run -i --tty load-generator --rm \
  --image=busybox --restart=Never \
  -- /bin/sh -c "while true; do wget -q -O- http://hpa-demo-deployment; sleep 0.01; done"



kubectl get hpa -w

kubectl describe deploy hpa-demo-deployment

kubectl get hpa

kubectl get events



----------------------------------------------------------Further notes will be added soon-----------------------------------------------------

